{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3rd_week_Improving the Fashion classifier with convolutions.ipynb","provenance":[],"authorship_tag":"ABX9TyNZiQIQWSqW9oaIRG/WDRf5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"9-ByLrHgcDBt"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBppiGMQe8C_","executionInfo":{"status":"ok","timestamp":1604279129642,"user_tz":-540,"elapsed":19452,"user":{"displayName":"김현준","photoUrl":"","userId":"08767726034577008058"}},"outputId":"ee61f4a0-82d3-4676-e9d8-1e4f8d6251b4","colab":{"base_uri":"https://localhost:8080/"}},"source":["import tensorflow as tf\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images / 255.0\n","test_images=test_images / 255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=5)\n","\n","test_loss = model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","Epoch 1/5\n","1875/1875 [==============================] - 3s 2ms/step - loss: 0.4933 - accuracy: 0.8266\n","Epoch 2/5\n","1875/1875 [==============================] - 3s 2ms/step - loss: 0.3759 - accuracy: 0.8629\n","Epoch 3/5\n","1875/1875 [==============================] - 3s 2ms/step - loss: 0.3355 - accuracy: 0.8767\n","Epoch 4/5\n","1875/1875 [==============================] - 3s 2ms/step - loss: 0.3137 - accuracy: 0.8845\n","Epoch 5/5\n","1875/1875 [==============================] - 3s 2ms/step - loss: 0.2945 - accuracy: 0.8910\n","313/313 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.8728\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_07JuExtfIoF","executionInfo":{"status":"ok","timestamp":1604279545474,"user_tz":-540,"elapsed":410096,"user":{"displayName":"김현준","photoUrl":"","userId":"08767726034577008058"}},"outputId":"f1c3515d-94c5-43e1-fbb1-b687e1ae79f4","colab":{"base_uri":"https://localhost:8080/"}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","model.fit(training_images, training_labels, epochs=5)\n","test_loss = model.evaluate(test_images, test_labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 64)        640       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 1600)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 128)               204928    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                1290      \n","=================================================================\n","Total params: 243,786\n","Trainable params: 243,786\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","1875/1875 [==============================] - 82s 44ms/step - loss: 0.4354 - accuracy: 0.8423\n","Epoch 2/5\n","1875/1875 [==============================] - 82s 44ms/step - loss: 0.2922 - accuracy: 0.8921\n","Epoch 3/5\n","1875/1875 [==============================] - 80s 43ms/step - loss: 0.2486 - accuracy: 0.9066\n","Epoch 4/5\n","1875/1875 [==============================] - 80s 43ms/step - loss: 0.2159 - accuracy: 0.9187\n","Epoch 5/5\n","1875/1875 [==============================] - 80s 43ms/step - loss: 0.1905 - accuracy: 0.9285\n","313/313 [==============================] - 4s 13ms/step - loss: 0.2659 - accuracy: 0.9013\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OY3AXdnqfhXZ"},"source":["It's likely gone up to about 93% on the training data and 91% on the validation data. \n","\n","That's significant, and a step in the right direction!\n","\n","Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n","\n","(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n","\n","Then, look at the code again, and see, step by step how the Convolutions were built:"]},{"cell_type":"markdown","metadata":{"id":"IQjW6Y_yflj6"},"source":["\n"," 첫 convolution은 하나의 tensor가 모든 것을 포함해야 하기 때문에 기존의 60000개의 28x28x1 item을 60000x28x28x1의 4d 리스트로 바꿔준다. 테스트 이미지에 대해서도 똑같이 해줘야 한다. 이렇게 하지 않으면 convolution이 shape를 인식하지 못하는 오류가 난다.\n","\n","\n","```\n","import tensorflow as tf\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"84ot23i4g1tr"},"source":["다음 단계는 모델 정의이다. Input layer 대신 conv layer를 처음 단계에 넣는다. 파라미터는 다음 4종류이다.\n","\n","1. convolution의 개수. 32의 배수가 좋다.\n","2. convolution의 size, 여기서는 3x3\n","3. 함수의 activaition 함수, 여기서는 relu\n","4. 첫번째 레이어에서 인풋 데이터의 shape\n","\n","\n","```\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"9V5EtX6RhUep"},"source":["Add another convolution\n","\n","\n","\n","```\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"zTeUyIoWhW9G"},"source":["Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n","\n","```\n","  tf.keras.layers.Flatten(),\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"qp4tlVxThY3M"},"source":["The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n","\n","\n","\n","```\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"0yUypUXwharN"},"source":["Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n","\n","\n","\n","```\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=5)\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(test_acc)\n","```\n"]},{"cell_type":"code","metadata":{"id":"B7WDXBtefOxq","executionInfo":{"status":"ok","timestamp":1604279840526,"user_tz":-540,"elapsed":54955,"user":{"displayName":"김현준","photoUrl":"","userId":"08767726034577008058"}},"outputId":"6379626c-26de-410b-9198-c3a120ec0cc3","colab":{"base_uri":"https://localhost:8080/"}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=10)\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(test_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Epoch 1/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.1519 - accuracy: 0.9549\n","Epoch 2/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0511 - accuracy: 0.9842\n","Epoch 3/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0320 - accuracy: 0.9902\n","Epoch 4/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0222 - accuracy: 0.9929\n","Epoch 5/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0136 - accuracy: 0.9955\n","Epoch 6/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0107 - accuracy: 0.9966\n","Epoch 7/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0082 - accuracy: 0.9972\n","Epoch 8/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0071 - accuracy: 0.9976\n","Epoch 9/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0054 - accuracy: 0.9982\n","Epoch 10/10\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0050 - accuracy: 0.9984\n","313/313 [==============================] - 1s 2ms/step - loss: 0.0523 - accuracy: 0.9856\n","0.9855999946594238\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ejLNWUHlhtga"},"source":[""],"execution_count":null,"outputs":[]}]}